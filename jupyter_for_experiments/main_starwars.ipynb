{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertForSequenceClassification,BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarWars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../data/raw/StarWarsEpisodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ep4 = os.path.join(base_dir,\"SW_EpisodeIV.txt\")\n",
    "folder_ep5 = os.path.join(base_dir,\"SW_EpisodeV.txt\")\n",
    "folder_ep6 = os.path.join(base_dir,\"SW_EpisodeVI.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ep4 = pd.read_csv(folder_ep4, sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep5 = pd.read_csv(folder_ep5, sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep6 = pd.read_csv(folder_ep6, sep =' ', header=0, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>Did you hear that?  They've shut down the main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>We're doomed!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>There'll be no escape for the Princess this time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>What's that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>I should have known better than to trust the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>Oh, no!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>Oh, my!  Artoo!  Can you hear me?  Say somethi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>TECHNICIAN</td>\n",
       "      <td>We'll get to work on him right away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>THREEPIO</td>\n",
       "      <td>You must repair him!  Sir, if any of my circui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>LUKE</td>\n",
       "      <td>He'll be all right.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       character                                           dialogue\n",
       "1       THREEPIO  Did you hear that?  They've shut down the main...\n",
       "2       THREEPIO                                      We're doomed!\n",
       "3       THREEPIO  There'll be no escape for the Princess this time.\n",
       "4       THREEPIO                                       What's that?\n",
       "5       THREEPIO  I should have known better than to trust the l...\n",
       "...          ...                                                ...\n",
       "1006        LUKE                                            Oh, no!\n",
       "1007    THREEPIO  Oh, my!  Artoo!  Can you hear me?  Say somethi...\n",
       "1008  TECHNICIAN               We'll get to work on him right away.\n",
       "1009    THREEPIO  You must repair him!  Sir, if any of my circui...\n",
       "1010        LUKE                                He'll be all right.\n",
       "\n",
       "[1010 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ep4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.concat([df_ep4['character'],df_ep5['character'],df_ep6['character']]).tolist()\n",
    "X = pd.concat([df_ep4['dialogue'],df_ep5['dialogue'],df_ep6['dialogue']]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(Y)\n",
    "label_count = [sum(i == np.array(Y)) for i in labels]\n",
    "for i,(a,b) in enumerate(zip(labels,label_count)):\n",
    "    if b < 10:\n",
    "        labels[i] = \"Other\"\n",
    "labels = np.unique(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ACKBAR', 'BEN', 'BIGGS', 'COMMANDER', 'CREATURE', 'EMPEROR',\n",
       "       'GOLD LEADER', 'HAN', 'JABBA', 'LANDO', 'LEIA', 'LUKE', 'OFFICER',\n",
       "       'OWEN', 'Other', 'PIETT', 'RED LEADER', 'RIEEKAN', 'TARKIN',\n",
       "       'THREEPIO', 'TROOPER', 'VADER', 'WEDGE', 'YODA'], dtype='<U30')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2ind = {i:j for i,j in zip(labels,range(len(labels)))}\n",
    "ind2char = {j:i for i,j in zip(labels,range(len(labels)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = X.copy()\n",
    "new_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(new_x)):\n",
    "    \n",
    "    if Y[idx] in labels:\n",
    "        label_point = char2ind[Y[idx]]\n",
    "    else:\n",
    "        label_point = char2ind[\"Other\"]\n",
    "    new_y.append(label_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define your inputs and labels\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "labels = torch.tensor([1, 0])  # The ground truth labels for your examples\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Compute the loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, labels)\n",
    "\n",
    "# Backward pass and optimization step\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "sentence_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    pre_models = ['bert-base-nli-mean-tokens',\n",
    "                  'all-mpnet-base-v2',\n",
    "                  \"multi-qa-mpnet-base-dot-v1\"]\n",
    "   # sentence_model = SentenceTransformer(pre_models[2])#SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    encoded_inputs = tokenizer.batch_encode_plus(X, padding=True, truncation=True, return_tensors='pt')\n",
    "    logits = sentence_model(**encoded_inputs)[0]\n",
    "\n",
    "    new_x = sentence_model.encode(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoded_inputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(X, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer.batch_encode_plus(X, padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_names = movie_lines.iloc[:,0]\n",
    "# movie_names = movie_lines.iloc[:,1]\n",
    "# char_names = np.unique(list(set(char_names.values)))\n",
    "# movie_names = np.unique(list(set(movie_names.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCustomStarWarsDataset\u001b[39;00m(Dataset):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, X, Y,transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, target_transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/main_starwars.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m X\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomStarWarsDataset(Dataset):\n",
    "    def __init__(self, X, Y,transform=None, target_transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.X[idx]\n",
    "\n",
    "        label_point = self.Y[idx]\n",
    "\n",
    "        # print(\"data_point is:\",data_point)\n",
    "        # print(\"label_point is:\",label_point)\n",
    "        # 768 \n",
    "        # print(sentence_encoded,label_point)\n",
    "        return data_point, label_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSentenceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertSentenceClassifier, self).__init__()\n",
    "        self.dropout_rate = .1\n",
    "        self.lin1 = nn.Linear(768,256)\n",
    "        self.lin_layers = nn.ModuleList([nn.Linear(256,256) for i in range(4)])\n",
    "\n",
    "\n",
    "        self.lin2 = nn.Linear(256, len(labels))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x = nn.functional.relu(self.lin1(data))\n",
    "        x = nn.functional.dropout(x,self.dropout_rate)\n",
    "        for i in self.lin_layers:\n",
    "            x = nn.functional.relu(i(x))\n",
    "            x = nn.functional.dropout(x,self.dropout_rate)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        x = nn.functional.softmax(x,dim = 1)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,x_test,y_train,y_test = train_test_split(new_x,new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(max_leaf_nodes=25)\n",
    "# model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.score(x_train,y_train),model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomStarWarsDataset(new_x,new_y)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [2000, 523])\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=32,shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(val_set,batch_size=32,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertSentenceClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    _, predictions = torch.max(preds, dim=1)\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    global model\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        #loop = tqdm(train_loader, total=len(train_loader))\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "        for text, author_labels in train_loader:  # Assuming data_loader is set up to provide batches of data\n",
    "            data = text.to(device)\n",
    "            author_labels = author_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data = model(data)\n",
    "            # print(data.shape)\n",
    "            # print(\"data and author labels ::::: \",data,author_labels)\n",
    "            loss = nn.CrossEntropyLoss()(data,author_labels)\n",
    "            acc = accuracy(data, author_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += acc\n",
    "            num_batches += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            # loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            # loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_running_accuracy = 0.0\n",
    "        val_num_batches = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation during validation\n",
    "            for val_text, val_labels in val_loader:\n",
    "                val_data = val_text.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "\n",
    "                val_output = model(val_data)\n",
    "                val_acc = accuracy(val_output, val_labels)\n",
    "\n",
    "                val_running_accuracy += val_acc\n",
    "                val_num_batches += 1\n",
    "        avg_loss = running_loss / num_batches\n",
    "        avg_accuracy = running_accuracy / num_batches\n",
    "        val_avg_accuracy = val_running_accuracy / val_num_batches\n",
    "        if epoch%100 ==1: \n",
    "            print(f\"Validation Accuracy: {val_avg_accuracy:.4f} Accuracy: {avg_accuracy:.4f}\")\n",
    "            \n",
    "            print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "573cf0678d00c74445c44d580176cd85302f26e1d414916d8b632bc81d10db1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
