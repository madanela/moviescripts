{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertForSequenceClassification,BertTokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StarWars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../data/raw/StarWarsEpisodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_ep4 = os.path.join(base_dir,\"SW_EpisodeIV.txt\")\n",
    "folder_ep5 = os.path.join(base_dir,\"SW_EpisodeV.txt\")\n",
    "folder_ep6 = os.path.join(base_dir,\"SW_EpisodeVI.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ep4 = pd.read_csv(folder_ep4, sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep5 = pd.read_csv(folder_ep5, sep =' ', header=0, escapechar='\\\\')\n",
    "df_ep6 = pd.read_csv(folder_ep6, sep =' ', header=0, escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.concat([df_ep4['character'],df_ep5['character'],df_ep6['character']]).tolist()\n",
    "X = pd.concat([df_ep4['dialogue'],df_ep5['dialogue'],df_ep6['dialogue']]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(Y)\n",
    "label_count = [sum(i == np.array(Y)) for i in labels]\n",
    "for i,(a,b) in enumerate(zip(labels,label_count)):\n",
    "    if b < 10:\n",
    "        labels[i] = \"Other\"\n",
    "labels = np.unique(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2ind = {i:j for i,j in zip(labels,range(len(labels)))}\n",
    "ind2char = {j:i for i,j in zip(labels,range(len(labels)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = X.copy()\n",
    "new_y = []\n",
    "for idx in range(len(new_x)):\n",
    "    \n",
    "    if Y[idx] in labels:\n",
    "        label_point = char2ind[Y[idx]]\n",
    "    else:\n",
    "        label_point = char2ind[\"Other\"]\n",
    "    new_y.append(label_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.unique([j for i in new_x for j in i.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "if os.path.exists(\"dict_of_words.pickle\"):\n",
    "    with open('dict_of_words.pickle', 'rb') as handle:\n",
    "        dict_of_synonyms = pickle.load(handle)\n",
    "else:\n",
    "    dict_of_synonyms = {x:ft.get_nearest_neighbors(x) for x in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "class BertSentenceClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertSentenceClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels,problem_type=\"multi_label_classification\")\n",
    "        self.dropout_rate = 0.1\n",
    "        self.lin1 = nn.Linear(768, 256)\n",
    "        self.lin_layers = nn.ModuleList([nn.Linear(256, 256) for i in range(4)])\n",
    "        self.lin2 = nn.Linear(256, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #pooler_output = bert_outputs[0]\n",
    "\n",
    "        # x = nn.functional.relu(self.lin1(pooler_output))\n",
    "        # x = nn.functional.dropout(x, self.dropout_rate)\n",
    "        # for lin_layer in self.lin_layers:\n",
    "        #     x = nn.functional.relu(lin_layer(x))\n",
    "        #     x = nn.functional.dropout(x, self.dropout_rate)\n",
    "        # logits = self.lin2(x)\n",
    "        return bert_outputs\n",
    "model = BertSentenceClassifier(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertSentenceClassifier(\n",
       "  (bert): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=24, bias=True)\n",
       "  )\n",
       "  (lin1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (lin_layers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (lin2): Linear(in_features=256, out_features=24, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:39:31.798484: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 21:39:32.451509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/arloopa/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/arloopa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "nltk.download('wordnet')\n",
    "def add_noise(text, p=0.2):\n",
    "    words = text.split()\n",
    "    num_noise_words = int(len(words) * p)\n",
    "    for i in range(num_noise_words):\n",
    "        idx = random.randint(0, len(words)-1)\n",
    "        word = words[idx]\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            synset = random.choice(synsets)\n",
    "            synonyms = synset.lemmas()\n",
    "            if random.random() < p/2:\n",
    "                synonym = random.choice(synonyms)\n",
    "                words[idx] = synonym.name()\n",
    "            # Replace word with nearest neighbor with probability p/2\n",
    "            else:\n",
    "                if word in dict_of_synonyms:\n",
    "                    \n",
    "                    nn = random.choice(dict_of_synonyms[word])\n",
    "                    print(\"number is nn: \",nn)\n",
    "                    words[idx] = nn[random.randint(0, len(nn)-1)]\n",
    "                else:\n",
    "                    synonym = random.choice(synonyms)\n",
    "                    words[idx] = synonym.name()\n",
    "    print(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = add_noise(str(self.X[idx]), p = .8)\n",
    "        label = self.y[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            add_special_tokens=True, \n",
    "            max_length=512, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_attention_mask=True, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        return input_ids, attention_mask, torch.tensor(label)\n",
    "\n",
    "# Assuming X is a list of texts and y is a list of labels\n",
    "X = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "y = [1, 0]\n",
    "\n",
    "# Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the dataset\n",
    "dataset = TextClassificationDataset(new_x, new_y, tokenizer)\n",
    "\n",
    "# Create the data loader\n",
    "batch_size = 4\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [2000, 523])\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=4,shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(val_set,batch_size=4,shuffle=True,drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f403e1ab760>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(preds, labels):\n",
    "#     _, predictions = torch.max(preds, dim=1)\n",
    "#     correct = (predictions == labels).sum().item()\n",
    "#     return correct / len(labels)\n",
    "# def train(model, optimizer, train_loader,val_loader, num_epochs):\n",
    "#     model = model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         running_accuracy = 0.0\n",
    "#         num_batches = 0\n",
    "#         loop = tqdm(train_loader, total=len(train_loader))\n",
    "\n",
    "#         for input_ids, attention_mask, labels in loop:\n",
    "#             input_ids = input_ids.to(device)\n",
    "#             attention_mask = attention_mask.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             loss = criterion(outputs.logits, labels)\n",
    "#             acc = accuracy(outputs.logits, labels)\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             running_accuracy += acc\n",
    "#             num_batches += 1\n",
    "#             loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "#             loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#         epoch_loss = running_loss / num_batches\n",
    "#         epoch_accuracy = running_accuracy / num_batches\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"saved_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    _, predictions = torch.max(preds, dim=1)\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader,num_epochs):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Training step\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, total=len(train_loader))\n",
    "        for input_ids, attention_mask, labels in loop:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            acc = accuracy(outputs.logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += acc\n",
    "            num_batches += 1\n",
    "            loop.set_description(f\"Epoch [{epoch + 1}] (Training)\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / num_batches\n",
    "        epoch_accuracy = running_accuracy / num_batches\n",
    "        print(f\"Epoch [{epoch+1}] (Training) Loss: {epoch_loss:.4f} Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Evaluation step\n",
    "        running_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in val_loader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                acc = accuracy(outputs.logits, labels)\n",
    "\n",
    "                running_accuracy += acc\n",
    "                num_batches += 1\n",
    "\n",
    "        val_accuracy = running_accuracy / num_batches\n",
    "        print(f\"Epoch [{epoch+1}] (Validation) Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shuttle', 'Tydirium,', 'what', 'be', 'your', 'freight', 'and', 'destination?']\n",
      "[\"I'm\", 'go', 'to', 'shut', 'down.On', 'everything', 'but', 'the', 'emergency', 'power.It', 'systems.']\n",
      "['atomic_number_53', 'hope', 'you', 'know', 'what', \"you're\", 'doing.']\n",
      "['Artoo-Detoo!', \"It's\", 'you!', \"It's\", 'you!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1] (Training):   0%|          | 1/500 [00:02<17:29,  2.10s/it, loss=3.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'that', 'is', 'your', 'destiny.']\n",
      "['You', 'are', 'unwise', 'to', 'downcast', 'your', 'defenses.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model,optimizer, train_loader,val_loader,\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loop \u001b[39m=\u001b[39m tqdm(train_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_ids, attention_mask, labels \u001b[39min\u001b[39;00m loop:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb Cell 22\u001b[0m in \u001b[0;36mTextClassificationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     text \u001b[39m=\u001b[39m add_noise(\u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX[idx]), p \u001b[39m=\u001b[39;49m \u001b[39m.8\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         text, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb Cell 22\u001b[0m in \u001b[0;36madd_noise\u001b[0;34m(text, p)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(words)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m word \u001b[39m=\u001b[39m words[idx]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m synsets \u001b[39m=\u001b[39m wordnet\u001b[39m.\u001b[39;49msynsets(word)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m synsets:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arloopa/Documents/Testings/moviescripts/jupyter_for_experiments/test_starwars_bert.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     synset \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(synsets)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:1750\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynsets\u001b[39m(\u001b[39mself\u001b[39m, lemma, pos\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m, check_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1744\u001b[0m     \u001b[39m\"\"\"Load all synsets with a given lemma and part of speech tag.\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m \u001b[39m    If no pos is specified, all synsets for all parts of speech\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m    will be loaded.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39m    If lang is specified, all the synsets associated with the lemma name\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[39m    of that language will be returned.\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1750\u001b[0m     lemma \u001b[39m=\u001b[39m lemma\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m   1752\u001b[0m     \u001b[39mif\u001b[39;00m lang \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1753\u001b[0m         get_synset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msynset_from_pos_and_offset\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "train(model,optimizer, train_loader,val_loader,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"saved_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define your inputs and labels\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "labels = torch.tensor([1, 0])  # The ground truth labels for your examples\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Compute the loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, labels)\n",
    "\n",
    "# Backward pass and optimization step\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = BertSentenceClassifier(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_state_dict(torch.load(\"saved_model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
